{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. Please check the pdf file for more details.*\n",
    "\n",
    "In this exercise you will:\n",
    "    \n",
    "- implement the **forward** and **backward** operations for different layers in neural networks, the layers including:\n",
    "  - fully-connected layer\n",
    "  - pooling layer\n",
    "  - relu layer\n",
    "  - convolution layer\n",
    "- implement stochastic gradient descent with momentum for optimization\n",
    "- implement neural networks for classification on the mnist digit classification dataset\n",
    "\n",
    "Please note that **YOU CANNOT USE ANY MACHINE LEARNING PACKAGE SUCH AS SKLEARN** for any homework, unless you are asked.\n",
    "\n",
    "This homework is modified from the assignment2 of [cs231n](http://cs231n.stanford.edu/) in stanford."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fc_net import *\n",
    "from data_utils import get_MNIST_data\n",
    "from gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (50000, 1, 28, 28)\n",
      "y_train:  (50000,)\n",
      "X_val:  (10000, 1, 28, 28)\n",
      "y_val:  (10000,)\n",
      "X_test:  (10000, 1, 28, 28)\n",
      "y_test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) mnist data.\n",
    "\n",
    "data = get_MNIST_data()\n",
    "for k, v in data.items():\n",
    "    print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 70 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(data['y_train'] == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(data['X_train'][idx].reshape((28, 28)))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: foward\n",
    "Open the file `layers.py` and implement the `affine_forward` function.\n",
    "\n",
    "Once you are done you can test your implementaion by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.49834967 1.70660132 1.91485297]\n",
      " [3.25553199 3.5141327  3.77273342]]\n",
      "Testing affine_forward function:\n",
      "difference: 9.769847728806635e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "print(out)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: {}'.format(rel_error(out, correct_out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: backward\n",
    "Now implement the `affine_backward` function and test your implementation using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "Testing affine_backward function:\n",
      "dx error: 5.588978358251405e-11\n",
      "dw error: 5.255960522091556e-11\n",
      "db error: 5.652042049296191e-11\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: {}'.format(rel_error(dx_num, dx)))\n",
    "print('dw error: {}'.format(rel_error(dw_num, dw)))\n",
    "print('db error: {}'.format(rel_error(db_num, db)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU layer: forward\n",
    "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference: 4.999999798022158e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: {}'.format(rel_error(out, correct_out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU layer: backward\n",
    "Now implement the backward pass for the ReLU activation function in the `relu_backward` function and test your implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error: 3.275636144741523e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: {}'.format(rel_error(dx_num, dx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1: \n",
    "\n",
    "We've only asked you to implement ReLU, but there are a number of different activation functions that one could use in neural networks, each with its pros and cons. In particular, an issue commonly seen with activation functions is getting zero (or close to zero) gradient flow during backpropagation. Which of the following activation functions have this problem? If you consider these functions in the one dimensional case, what types of input would lead to this behaviour?\n",
    "1. Sigmoid\n",
    "2. ReLU\n",
    "3. [Leaky ReLU](https://cs231n.github.io/neural-networks-1/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Sandwich\" layers\n",
    "There are some common patterns of layers that are frequently used in neural nets. For example, affine layers are frequently followed by a ReLU nonlinearity. To make these common patterns easy, we define several convenience layers in the file `layer_utils.py`.\n",
    "\n",
    "For now take a look at the `affine_relu_forward` and `affine_relu_backward` functions, and run the following to numerically gradient check the backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_relu_forward and affine_relu_backward:\n",
      "dx error: 3.249745190645656e-10\n",
      "dw error: 2.906018860121998e-10\n",
      "db error: 5.200119499766052e-10\n"
     ]
    }
   ],
   "source": [
    "from layer_utils import affine_relu_forward, affine_relu_backward\n",
    "\n",
    "x = np.random.randn(2, 3, 4)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "\n",
    "out, cache = affine_relu_forward(x, w, b)\n",
    "dx, dw, db = affine_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "print('Testing affine_relu_forward and affine_relu_backward:')\n",
    "print('dx error: {}'.format(rel_error(dx_num, dx)))\n",
    "print('dw error: {}'.format(rel_error(dw_num, dw)))\n",
    "print('db error: {}'.format(rel_error(db_num, db)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss layers: Softmax and SVM\n",
    "We give some loss functions here. You should still make sure you understand how they work by looking at the implementations in `layers.py`.\n",
    "\n",
    "You can make sure that the implementations are correct by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing svm_loss:\n",
      "loss: 9.00260942299131\n",
      "dx error: 8.182894472887002e-10\n",
      "\n",
      "Testing softmax_loss:\n",
      "loss: 2.3028464493765664\n",
      "dx error: 6.908425146163007e-09\n"
     ]
    }
   ],
   "source": [
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: svm_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = svm_loss(x, y)\n",
    "\n",
    "# Test svm_loss function. Loss should be around 9 and dx error should be 1e-9\n",
    "print('Testing svm_loss:')\n",
    "print('loss: {}'.format(loss))\n",
    "print('dx error: {}'.format(rel_error(dx_num, dx)))\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be 2.3 and dx error should be 1e-8\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: {}'.format(loss))\n",
    "print('dx error: {}'.format(rel_error(dx_num, dx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-layer network\n",
    "\n",
    "Now you will reimplement the two layer network using these modular implementations.\n",
    "\n",
    "Open the file `fc_net.py` and complete the implementation of the `TwoLayerNet` class. This class will serve as a model for the other networks you will implement in this assignment, so read through it to make sure you understand the API. You can run the cell below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing initialization ... \n",
      "Testing test-time forward pass ... \n",
      "Testing training loss (no regularization)\n",
      "Running numeric gradient check with reg = 0.0\n",
      "dict_keys(['b2', 'W2', 'b1', 'W1'])\n",
      "W1 relative error: 1.2165499269182414e-08\n",
      "W2 relative error: 3.4803693682531243e-10\n",
      "b1 relative error: 6.5485474139109215e-09\n",
      "b2 relative error: 4.3291413857436005e-10\n",
      "Running numeric gradient check with reg = 0.7\n",
      "dict_keys(['b2', 'W2', 'b1', 'W1'])\n",
      "W1 relative error: 8.175466200078585e-07\n",
      "W2 relative error: 2.8508696990815807e-08\n",
      "b1 relative error: 1.0895946645012713e-09\n",
      "b2 relative error: 9.089615724390711e-10\n"
     ]
    }
   ],
   "source": [
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-2\n",
    "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "print('Testing test-time forward pass ... ')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print('Testing training loss (no regularization)')\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 26.5948426952\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
    "\n",
    "for reg in [0.0, 0.7]:\n",
    "    print('Running numeric gradient check with reg = {}'.format(reg))\n",
    "    model.reg = reg\n",
    "    loss, grads = model.loss(X, y)\n",
    "    print(grads.keys())\n",
    "    for name in sorted(grads):\n",
    "        f = lambda _: model.loss(X, y)[0]\n",
    "        grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "        print('{} relative error: {}'.format(name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver\n",
    "\n",
    "Open the file `solver.py` and read through it to familiarize yourself with the API. After doing so, use a `Solver` instance to train a `TwoLayerNet` that achieves at least `95%` accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet()\n",
    "solver = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Use a Solver instance to train a TwoLayerNet that achieves at least  #\n",
    "# 95% accuracy on the validation set.                                        #\n",
    "##############################################################################\n",
    "# begin answer\n",
    "# end answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to visualize training loss and train / val accuracy\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer network\n",
    "Next you will implement a fully-connected network with an arbitrary number of hidden layers.\n",
    "\n",
    "Read through the `FullyConnectedNet` class in the file `fc_net.py`.\n",
    "\n",
    "Implement the initialization, the forward pass, and the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial loss and gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization. Do the initial losses seem reasonable?\n",
    "\n",
    "For gradient checking, you should expect to see errors around 1e-6 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "    print('Running check with reg = {}'.format(reg))\n",
    "    model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
    "\n",
    "    loss, grads = model.loss(X, y)\n",
    "    print('Initial loss: {}'.format(loss))\n",
    "\n",
    "    for name in sorted(grads):\n",
    "        f = lambda _: model.loss(X, y)[0]\n",
    "        grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "        print('{} relative error: {}'.format(name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another sanity check, make sure you can overfit a small dataset of 50 images. First we will try a three-layer network with 100 units in each hidden layer. You will need to tweak the learning rate and initialization scale, but you should be able to overfit and achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use a three-layer Net to overfit 50 training examples.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 1e-1\n",
    "learning_rate = 1e-1\n",
    "# tweak these two parameters\n",
    "# begin answer\n",
    "# end answer\n",
    "model = FullyConnectedNet([100, 100],\n",
    "              weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to use a five-layer network with 100 units on each layer to overfit 50 training examples. Again you will have to adjust the learning rate and weight initialization, but you should be able to achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use a five-layer Net to overfit 50 training examples.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 1e-1\n",
    "learning_rate = 1e-1\n",
    "# begin answer\n",
    "# end answer\n",
    "model = FullyConnectedNet([100, 100, 100, 100],\n",
    "            weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "               )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline question 2: \n",
    "Did you notice anything about the comparative difficulty of training the three-layer net vs training the five layer net?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update rules\n",
    "So far we have used vanilla stochastic gradient descent (SGD) as our update rule in assignment2.\n",
    "```\n",
    "# Vanilla update\n",
    "x += - learning_rate * dx\n",
    "```\n",
    "\n",
    "More sophisticated update rules can make it easier to train deep networks. We will implement a few of the most commonly used update rules and compare them to vanilla SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD+Momentum\n",
    "Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochstic gradient descent.\n",
    "\n",
    "```python\n",
    "# Momentum update\n",
    "v = mu * v - learning_rate * dx # integrate velocity\n",
    "x += v # integrate position\n",
    "```\n",
    "\n",
    "Now open the file `optim.py` and read the documentation at the top of the file to make sure you understand the API. The implementation of vanilla SGD is provided. Implement the SGD+momentum update rule in the function `sgd_momentum` and run the following to check your implementation. You should see errors less than 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optim import sgd_momentum\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-3, 'velocity': v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "print('next_w error: {}'.format(rel_error(next_w, expected_next_w)))\n",
    "print('velocity error: {}'.format(rel_error(expected_velocity, config['velocity'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have done so, run the following to train a six-layer network with both SGD and SGD+momentum. You should see the SGD+momentum update rule converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule in ['sgd', 'sgd_momentum']:\n",
    "    print('running with {}'.format(update_rule))\n",
    "    model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "    solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': 0.2,\n",
    "                  },\n",
    "                  verbose=True)\n",
    "    solvers[update_rule] = solver\n",
    "    solver.train()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in solvers.items():\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "\n",
    "for i in [1, 2, 3]:\n",
    "    plt.subplot(3, 1, i)\n",
    "    plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Convolutional Networks\n",
    "\n",
    "So far we have worked with deep fully-connected networks, using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, but in practice all state-of-the-art results use convolutional networks instead.\n",
    "\n",
    "First you will implement several layer types that are used in convolutional networks. You will then use these layers to train a convolutional network on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution: Naive forward pass\n",
    "The core of a convolutional network is the convolution operation. In the file `layers.py`, implement the forward pass for the convolution layer in the function `conv_forward_naive`. \n",
    "\n",
    "You don't have to worry too much about efficiency at this point; just write the code in whatever way you find most clear.\n",
    "\n",
    "You can test your implementation by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "out, _ = conv_forward_naive(x, w, b, conv_param)\n",
    "correct_out = np.array([[[[[-0.08759809, -0.10987781],\n",
    "                           [-0.18387192, -0.2109216 ]],\n",
    "                          [[ 0.21027089,  0.21661097],\n",
    "                           [ 0.22847626,  0.23004637]],\n",
    "                          [[ 0.50813986,  0.54309974],\n",
    "                           [ 0.64082444,  0.67101435]]],\n",
    "                         [[[-0.98053589, -1.03143541],\n",
    "                           [-1.19128892, -1.24695841]],\n",
    "                          [[ 0.69108355,  0.66880383],\n",
    "                           [ 0.59480972,  0.56776003]],\n",
    "                          [[ 2.36270298,  2.36904306],\n",
    "                           [ 2.38090835,  2.38247847]]]]])\n",
    "\n",
    "# Compare your output to ours; difference should be around 1e-8\n",
    "print('Testing conv_forward_naive')\n",
    "print('difference: {}'.format(rel_error(out, correct_out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aside: Image processing via convolutions\n",
    "\n",
    "As fun way to both check your implementation and gain a better understanding of the type of operation that convolutional layers can perform, we will set up an input containing two images and manually set up filters that perform common image processing operations (grayscale conversion and edge detection). The convolution forward pass will apply these operations to each of the input images. We can then visualize the results as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from imageio import imread\n",
    "\n",
    "def imresize(image, size):\n",
    "    return np.array(Image.fromarray(image).resize(size))\n",
    "\n",
    "kitten, puppy = imread('kitten.jpg'), imread('puppy.jpg')\n",
    "# kitten is wide, and puppy is already square\n",
    "d = kitten.shape[1] - kitten.shape[0]\n",
    "kitten_cropped = kitten[:, d//2:-d//2, :]\n",
    "\n",
    "img_size = 200   # Make this smaller if it runs too slow\n",
    "x = np.zeros((2, 3, img_size, img_size))\n",
    "x[0, :, :, :] = imresize(puppy, (img_size, img_size)).transpose((2, 0, 1))\n",
    "x[1, :, :, :] = imresize(kitten_cropped, (img_size, img_size)).transpose((2, 0, 1))\n",
    "\n",
    "# Set up a convolutional weights holding 2 filters, each 3x3\n",
    "w = np.zeros((2, 3, 3, 3))\n",
    "\n",
    "# The first filter converts the image to grayscale.\n",
    "# Set up the red, green, and blue channels of the filter.\n",
    "w[0, 0, :, :] = [[0, 0, 0], [0, 0.3, 0], [0, 0, 0]]\n",
    "w[0, 1, :, :] = [[0, 0, 0], [0, 0.6, 0], [0, 0, 0]]\n",
    "w[0, 2, :, :] = [[0, 0, 0], [0, 0.1, 0], [0, 0, 0]]\n",
    "\n",
    "# Second filter detects horizontal edges in the blue channel.\n",
    "w[1, 2, :, :] = [[1, 2, 1], [0, 0, 0], [-1, -2, -1]]\n",
    "\n",
    "# Vector of biases. We don't need any bias for the grayscale\n",
    "# filter, but for the edge detection filter we want to add 128\n",
    "# to each output so that nothing is negative.\n",
    "b = np.array([0, 128])\n",
    "\n",
    "# Compute the result of convolving each input in x with each filter in w,\n",
    "# offsetting by b, and storing the results in out.\n",
    "out, _ = conv_forward_naive(x, w, b, {'stride': 1, 'pad': 1})\n",
    "\n",
    "def imshow_noax(img, normalize=True):\n",
    "    \"\"\" Tiny helper to show images as uint8 and remove axis labels \"\"\"\n",
    "    if normalize:\n",
    "        img_max, img_min = np.max(img), np.min(img)\n",
    "        img = 255.0 * (img - img_min) / (img_max - img_min)\n",
    "    plt.imshow(img.astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "# Show the original images and the results of the conv operation\n",
    "plt.subplot(2, 3, 1)\n",
    "imshow_noax(puppy, normalize=False)\n",
    "plt.title('Original image')\n",
    "plt.subplot(2, 3, 2)\n",
    "imshow_noax(out[0, 0])\n",
    "plt.title('Grayscale')\n",
    "plt.subplot(2, 3, 3)\n",
    "imshow_noax(out[0, 1])\n",
    "plt.title('Edges')\n",
    "plt.subplot(2, 3, 4)\n",
    "imshow_noax(kitten_cropped, normalize=False)\n",
    "plt.subplot(2, 3, 5)\n",
    "imshow_noax(out[1, 0])\n",
    "plt.subplot(2, 3, 6)\n",
    "imshow_noax(out[1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution: Naive backward pass\n",
    "Implement the backward pass for the convolution operation in the function `conv_backward_naive` in the file `layers.py`. Again, you don't need to worry too much about computational efficiency.\n",
    "\n",
    "When you are done, run the following to check your backward pass with a numeric gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(4, 3, 5, 5)\n",
    "w = np.random.randn(2, 3, 3, 3)\n",
    "b = np.random.randn(2,)\n",
    "dout = np.random.randn(4, 2, 5, 5)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_forward_naive(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_forward_naive(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_forward_naive(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "out, cache = conv_forward_naive(x, w, b, conv_param)\n",
    "dx, dw, db = conv_backward_naive(dout, cache)\n",
    "\n",
    "# Your errors should be around 1e-9'\n",
    "print('Testing conv_backward_naive function')\n",
    "print('dx error: {}'.format(rel_error(dx, dx_num)))\n",
    "print('dw error: {}'.format(rel_error(dw, dw_num)))\n",
    "print('db error: {}'.format(rel_error(db, db_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max pooling: Naive forward\n",
    "Implement the forward pass for the max-pooling operation in the function `max_pool_forward_naive` in the file `cs231n/layers.py`. Again, don't worry too much about computational efficiency.\n",
    "\n",
    "Check your implementation by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "pool_param = {'pool_width': 2, 'pool_height': 2, 'stride': 2}\n",
    "\n",
    "out, _ = max_pool_forward_naive(x, pool_param)\n",
    "\n",
    "correct_out = np.array([[[[-0.26315789, -0.24842105],\n",
    "                          [-0.20421053, -0.18947368]],\n",
    "                         [[-0.14526316, -0.13052632],\n",
    "                          [-0.08631579, -0.07157895]],\n",
    "                         [[-0.02736842, -0.01263158],\n",
    "                          [ 0.03157895,  0.04631579]]],\n",
    "                        [[[ 0.09052632,  0.10526316],\n",
    "                          [ 0.14947368,  0.16421053]],\n",
    "                         [[ 0.20842105,  0.22315789],\n",
    "                          [ 0.26736842,  0.28210526]],\n",
    "                         [[ 0.32631579,  0.34105263],\n",
    "                          [ 0.38526316,  0.4       ]]]])\n",
    "\n",
    "# Compare your output with ours. Difference should be around 1e-8.\n",
    "print('Testing max_pool_forward_naive function:')\n",
    "print('difference: {}'.format(rel_error(out, correct_out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max pooling: Naive backward\n",
    "Implement the backward pass for the max-pooling operation in the function `max_pool_backward_naive` in the file `cs231n/layers.py`. You don't need to worry about computational efficiency.\n",
    "\n",
    "Check your implementation with numeric gradient checking by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(3, 2, 8, 8)\n",
    "dout = np.random.randn(3, 2, 4, 4)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: max_pool_forward_naive(x, pool_param)[0], x, dout)\n",
    "\n",
    "out, cache = max_pool_forward_naive(x, pool_param)\n",
    "dx = max_pool_backward_naive(dout, cache)\n",
    "\n",
    "# Your error should be around 1e-12\n",
    "print('Testing max_pool_backward_naive function:')\n",
    "print('dx error: {}'.format(rel_error(dx, dx_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast layers\n",
    "Making convolution and pooling layers fast can be challenging. To spare you the pain, we've provided fast implementations of the forward and backward passes for convolution and pooling layers in the file `fast_layers.py`.\n",
    "\n",
    "The API for the fast versions of the convolution and pooling layers is exactly the same as the naive versions that you implemented above: the forward pass receives data, weights, and parameters and produces outputs and a cache object; the backward pass recieves upstream derivatives and the cache object and produces gradients with respect to the data and weights.\n",
    "\n",
    "**NOTE:** The fast implementation for pooling will only perform optimally if the pooling regions are non-overlapping and tile the input. If these conditions are not met then the fast pooling implementation will not be much faster than the naive implementation.\n",
    "\n",
    "You can compare the performance of the naive and fast versions of these layers by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_layers import conv_forward_fast, conv_backward_fast\n",
    "from time import time\n",
    "\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(100, 3, 31, 31)\n",
    "w = np.random.randn(25, 3, 3, 3)\n",
    "b = np.random.randn(25,)\n",
    "dout = np.random.randn(100, 25, 16, 16)\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "\n",
    "t0 = time()\n",
    "out_naive, cache_naive = conv_forward_naive(x, w, b, conv_param)\n",
    "t1 = time()\n",
    "out_fast, cache_fast = conv_forward_fast(x, w, b, conv_param)\n",
    "t2 = time()\n",
    "\n",
    "print('Testing conv_forward_fast:')\n",
    "print('Naive: {}s'.format(t1 - t0))\n",
    "print('Fast: {}s'.format(t2 - t1))\n",
    "print('Speedup: {}x'.format((t1 - t0) / (t2 - t1)))\n",
    "print('Difference: {}s'.format(rel_error(out_naive, out_fast)))\n",
    "\n",
    "t0 = time()\n",
    "dx_naive, dw_naive, db_naive = conv_backward_naive(dout, cache_naive)\n",
    "t1 = time()\n",
    "dx_fast, dw_fast, db_fast = conv_backward_fast(dout, cache_fast)\n",
    "t2 = time()\n",
    "\n",
    "print('\\nTesting conv_backward_fast:')\n",
    "print('Naive: {}s'.format(t1 - t0))\n",
    "print('Fast: {}s'.format(t2 - t1))\n",
    "print('Speedup: {}x'.format((t1 - t0) / (t2 - t1)))\n",
    "print('dx difference: {}'.format(rel_error(dx_naive, dx_fast)))\n",
    "print('dw difference: {}'.format(rel_error(dw_naive, dw_fast)))\n",
    "print('db difference: {}'.format(rel_error(db_naive, db_fast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_layers import max_pool_forward_fast, max_pool_backward_fast\n",
    "\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(100, 3, 32, 32)\n",
    "dout = np.random.randn(100, 3, 16, 16)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "t0 = time()\n",
    "out_naive, cache_naive = max_pool_forward_naive(x, pool_param)\n",
    "t1 = time()\n",
    "out_fast, cache_fast = max_pool_forward_fast(x, pool_param)\n",
    "t2 = time()\n",
    "\n",
    "print('Testing pool_forward_fast:')\n",
    "print('Naive: {}s'.format(t1 - t0))\n",
    "print('fast: {}s'.format(t2 - t1))\n",
    "print('speedup: {}x'.format((t1 - t0) / (t2 - t1)))\n",
    "print('difference: {}'.format(rel_error(out_naive, out_fast)))\n",
    "\n",
    "t0 = time()\n",
    "dx_naive = max_pool_backward_naive(dout, cache_naive)\n",
    "t1 = time()\n",
    "dx_fast = max_pool_backward_fast(dout, cache_fast)\n",
    "t2 = time()\n",
    "\n",
    "print('\\nTesting pool_backward_fast:')\n",
    "print('Naive: {}s'.format(t1 - t0))\n",
    "print('fast: {}s'.format(t2 - t1))\n",
    "print('speedup: {}x'.format((t1 - t0) / (t2 - t1)))\n",
    "print('dx difference: {}'.format(rel_error(dx_naive, dx_fast)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional \"sandwich\" layers\n",
    "Previously we introduced the concept of \"sandwich\" layers that combine multiple operations into commonly used patterns. In the file `layer_utils.py` you will find sandwich layers that implement a few commonly used patterns for convolutional networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer_utils import conv_relu_pool_forward, conv_relu_pool_backward\n",
    "\n",
    "x = np.random.randn(2, 3, 16, 16)\n",
    "w = np.random.randn(3, 3, 3, 3)\n",
    "b = np.random.randn(3,)\n",
    "dout = np.random.randn(2, 3, 8, 8)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "out, cache = conv_relu_pool_forward(x, w, b, conv_param, pool_param)\n",
    "dx, dw, db = conv_relu_pool_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_relu_pool_forward(x, w, b, conv_param, pool_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_relu_pool_forward(x, w, b, conv_param, pool_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_relu_pool_forward(x, w, b, conv_param, pool_param)[0], b, dout)\n",
    "\n",
    "print('Testing conv_relu_pool')\n",
    "print('dx error: {}'.format(rel_error(dx_num, dx)))\n",
    "print('dw error: {}'.format(rel_error(dw_num, dw)))\n",
    "print('db error: {}'.format(rel_error(db_num, db)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three-layer ConvNet\n",
    "Now that you have implemented all the necessary layers, we can put them together into a simple convolutional network.\n",
    "\n",
    "Open the file `cnn.py` and complete the implementation of the `ThreeLayerConvNet` class. Run the following cells to help you debug:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check loss\n",
    "After you build a new network, one of the first things you should do is sanity check the loss. When we use the softmax loss, we expect the loss for random weights (and no regularization) to be about `log(C)` for `C` classes. When we add regularization this should go up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn import ThreeLayerConvNet\n",
    "model = ThreeLayerConvNet()\n",
    "\n",
    "N = 50\n",
    "X = np.random.randn(N, 1, 28, 28)\n",
    "y = np.random.randint(10, size=N)\n",
    "\n",
    "loss, grads = model.loss(X, y)\n",
    "print('Initial loss (no regularization): {}'.format(loss))\n",
    "\n",
    "model.reg = 0.5\n",
    "loss, grads = model.loss(X, y)\n",
    "print('Initial loss (with regularization): {}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient check\n",
    "After the loss looks reasonable, use numeric gradient checking to make sure that your backward pass is correct. When you use numeric gradient checking you should use a small amount of artifical data and a small number of neurons at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "input_dim = (3, 16, 16)\n",
    "reg = 0.0\n",
    "num_classes = 10\n",
    "np.random.seed(231)\n",
    "X = np.random.randn(num_inputs, *input_dim)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "model = ThreeLayerConvNet(num_filters=3, filter_size=3,\n",
    "                          input_dim=input_dim, hidden_dim=7,\n",
    "                          dtype=np.float64)\n",
    "loss, grads = model.loss(X, y)\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit small data\n",
    "A nice trick is to train your model with just a few training samples. You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "model = ThreeLayerConvNet(weight_scale=0.14)\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='sgd_momentum',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-2,\n",
    "                },\n",
    "                verbose=True, print_every=1)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the loss, training accuracy, and validation accuracy should show clear overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the net\n",
    "By training the three-layer convolutional network for one epoch, you should achieve greater than 95% accuracy on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ThreeLayerConvNet(weight_scale=0.1, hidden_dim=500, reg=0.001)\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                num_epochs=1, batch_size=50,\n",
    "                update_rule='sgd_momentum',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-2,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Filters\n",
    "You can visualize the first-layer convolutional filters from the trained network by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis_utils import visualize_grid\n",
    "\n",
    "grid = visualize_grid(model.params['W1'].transpose(0, 2, 3, 1))\n",
    "plt.imshow(grid.astype('uint8').reshape((grid.shape[0], grid.shape[1])))\n",
    "plt.axis('off')\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment!\n",
    "Experiment and try to get the best performance that you can on MNIST using a CNN. Here are some ideas to get you started:\n",
    "\n",
    "### Things you should try:\n",
    "- Filter size: Above we used 7x7; this makes pretty pictures but smaller filters may be more efficient\n",
    "- Number of filters: Above we used 32 filters. Do more or fewer do better?\n",
    "- Network architecture: The network above has two layers of trainable parameters. Can you do better with a deeper network? You can implement alternative architectures in the file `cnn.py`. Some good architectures to try include:\n",
    "    - [conv-relu-pool]xN - conv - relu - [affine]xM - [softmax or SVM]\n",
    "    - [conv-relu-pool]XN - [affine]XM - [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN - [affine]xM - [softmax or SVM]\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the course-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these.\n",
    "\n",
    "- Alternative update steps: For the assignment we implemented SGD+momentum; you could try alternatives like RMSprop and Adam.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "\n",
    "\n",
    "### What we expect\n",
    "At the very least, you should be able to train a CNN that gets at least 98% accuracy on the validation set. This is just a lower bound - if you are careful it should be possible to get accuracies higher than that! \n",
    "\n",
    "Have fun and happy training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# begin answer\n",
    "# end answer"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
